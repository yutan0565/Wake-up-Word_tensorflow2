{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yutan0565/Wake-up-Word_tensorflow2/blob/main/Wake_up_word_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFceqLTOE0ZZ",
    "outputId": "246d14c1-9604-4087-87cb-f698a4823cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python_speech_features in c:\\users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: playsound in c:\\users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.11.tar.gz (37 kB)\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (setup.py): started\n",
      "  Building wheel for pyaudio (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyaudio\n",
      "Failed to build pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "    Running setup.py install for pyaudio: started\n",
      "    Running setup.py install for pyaudio: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   command: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-wheel-7y4_g2u6'\n",
      "       cwd: C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-install-rkymhai7\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\n",
      "  Complete output (9 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  copying src\\pyaudio.py -> build\\lib.win-amd64-3.7\n",
      "  running build_ext\n",
      "  building '_portaudio' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyaudio\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-record-ihjo9ypo\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\Include\\pyaudio'\n",
      "         cwd: C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-install-rkymhai7\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\n",
      "    Complete output (9 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    copying src\\pyaudio.py -> build\\lib.win-amd64-3.7\n",
      "    running build_ext\n",
      "    building '_portaudio' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rkymhai7\\\\pyaudio_3e6424cbd9254abf8e2d3cc9b7c6c405\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-record-ihjo9ypo\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\Include\\pyaudio' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "# !pip install python_speech_features\n",
    "# !pip install playsound\n",
    "# !pip install -q tensorflow-model-optimization\n",
    "# !pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL10wX1siLGx"
   },
   "source": [
    "# 데이터 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "LaIMW3_FdLEV"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import python_speech_features\n",
    "from playsound import playsound\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow import lite\n",
    "\n",
    "#base_path = \"/content/drive/MyDrive/\"\n",
    "base_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "RwAiwzW6oNC4"
   },
   "outputs": [],
   "source": [
    "# MFCC 추출 해주기\n",
    "# https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=sooftware&logNo=221661644808 참고\n",
    "\n",
    "# frame 개수를 보여줌, output 세로\n",
    "num_mfcc = 40\n",
    "def get_librosa_mfcc(filepath):\n",
    "  samplie_rate = 16000\n",
    "  sig, sr = librosa.core.load(filepath, samplie_rate)\n",
    "  sig = sig[:20000]\n",
    "  print(sig.shape)\n",
    "  hop_len = 800  # sig / hop_len 만큼 크기의 output 이 나옴\n",
    "  # log mell 찾아 보기\n",
    "  # 120 만큼은 겹치게 된다.\n",
    "  mfccs = librosa.feature.mfcc(y = sig, sr = sr, hop_length =hop_len, n_mfcc = num_mfcc, n_fft = 512 )\n",
    "  return mfccs\n",
    "\n",
    "# output 가로\n",
    "len_mfcc = 26\n",
    "\n",
    "\n",
    "# n_mfcc : 음성데이터를 어느 단위로 쪼갤지 (사람은 20 ~ 40 ms 까지는 음소가 바뀌지 못함 - 말 자르는 가장 작은 단위 == frame_size\n",
    "# n_fft : frame의 length = window size,   잘린 음석이 n_ftt보다 작으면 0으로 Padding 해줌, \n",
    "   #n_fft는 winddow size보다 크거나 같아야함\n",
    "   # n_ftt = 8000 * 0.040 = 320\n",
    "# hop_legth : window 얼마 만큼씩 움질일 것인가\n",
    "\n",
    "  # n_mfcc = 40\n",
    "  # hop_length = 200  # 8000 * 0.040\n",
    "  # N_FFT = 320    # 8000 * 0.040\n",
    "\n",
    "# 이거 기준으로 나눈 다음에 Mel값을 뽑아서 Feature로 사용하게 된다.  (  50%는 겹치게 분할을 진행)\n",
    "# 각각의 frame에 대해서 Hamming Window 적용해서 연속성 맞춰주기 - Default 설정임\n",
    "# 각 프레임에 대하여 Fourier Transform 적용해서(FFT) 주파서 성분 알아내기\n",
    "# Mel Filter Banb(삶귀처럼, 주파수 증가할수록 큰 삼각형 filter가 생각다고 생각하기)\n",
    "# 여기까지하면 Mel-Spectrogram Feature가 추출된다.\n",
    "\n",
    "# Mel-Spectrogram 을 압축해서 표현해주는 DCT 연산 수행 -> Discrete Cosine Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "iigZ1Z0diKbl",
    "outputId": "0cb0ca6b-8a24-407a-b0bb-ecce4a39c67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./custum_dataset\n",
      ".ipynb_checkpoints\n",
      "hi_yutan\n",
      "no\n",
      "_background_noise_\n",
      "['hi_yutan', 'no']\n",
      "./custum_dataset/hi_yutan\n",
      "./custum_dataset/no\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 저장되어 있는 경로 ( class 별로 묶여 있는 곳)\n",
    "dataset_path = base_path + 'custum_dataset'\n",
    "print(dataset_path)\n",
    "\n",
    "# 폴더 안에 들어 있는 폴더명(class 이름 확인)\n",
    "for name in listdir(dataset_path): \n",
    "  if isdir( \"/\".join( [dataset_path,name ])) :  # 폴더가 존재 한다면\n",
    "    print(name)\n",
    "\n",
    "# 폴더명 = target ,  target이 들어가 있는 list 생성\n",
    "target_list = [ name for name in listdir(dataset_path) if isdir(\"/\".join( [dataset_path,name ]))]\n",
    "target_list.remove('_background_noise_')  # 배경음 제거 해주기\n",
    "target_list.remove('.ipynb_checkpoints')\n",
    "print(target_list)\n",
    "\n",
    "# filname, 을 쪽 모아서,\n",
    "filenames = []\n",
    "y = []\n",
    "for index, target in enumerate(target_list):\n",
    "    print('/'.join([dataset_path, target]))  # class 에 맞는 폴더 이름 넣어주기\n",
    "    filenames.append(listdir('/'.join([dataset_path, target])))\n",
    "    y.append(np.ones(len(filenames[index])) * index) \n",
    "\n",
    "# 하나로 쭉 나열 하기\n",
    "filenames = [item for sublist in filenames for item in sublist]\n",
    "y = [item for sublist in y for item in sublist]\n",
    "\n",
    "# file 모아둔거 한번 섞어 주기\n",
    "filenames_y = list(zip(filenames, y))\n",
    "random.shuffle(filenames_y)\n",
    "filenames, y = zip(*filenames_y)\n",
    "\n",
    "# train, valid, test 나눠 주기\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "\n",
    "val_set_size = int(len(filenames) * val_ratio)\n",
    "test_set_size = int(len(filenames) * test_ratio)\n",
    "\n",
    "filenames_val = filenames[:val_set_size]\n",
    "filenames_test = filenames[val_set_size:(val_set_size + test_set_size)]\n",
    "filenames_train = filenames[(val_set_size + test_set_size):]\n",
    "\n",
    "y_orig_val = y[:val_set_size]\n",
    "y_orig_test = y[val_set_size:(val_set_size + test_set_size)]\n",
    "y_orig_train = y[(val_set_size + test_set_size):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n",
      "24 24\n",
      "85 85\n"
     ]
    }
   ],
   "source": [
    "print(len(filenames_val), len(y_orig_val))\n",
    "print(len(filenames_test), len(y_orig_test))\n",
    "print(len(filenames_train), len(y_orig_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "Ir7V_q_VoNFb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n"
     ]
    }
   ],
   "source": [
    "# 지금 내가 가지고 있는 것을 다 변환 했을때  동일한 크기의 mfcc 파일이 나오는가, 이상한거는 버려주기\n",
    "prob_cnt = 0  \n",
    "x_test = []\n",
    "y_test = []\n",
    "for index, filename in enumerate(filenames_train):\n",
    "    \n",
    "    # Stop after 500\n",
    "    if index >= 500:\n",
    "        break\n",
    "    \n",
    "    # Create path from given filename and target item\n",
    "    path = join(dataset_path, target_list[int(y_orig_train[index])], filename)\n",
    "    \n",
    "    # Create MFCCs\n",
    "    mfccs = get_librosa_mfcc(path)\n",
    "    print(mfccs.shape)\n",
    "    if mfccs.shape[1] == len_mfcc:    # 지금은 40으로 설정되어 있음, ///  길이가 부족하면 나중에 버려야함\n",
    "        x_test.append(mfccs)\n",
    "        y_test.append(y_orig_train[index])\n",
    "    else:\n",
    "        print('Dropped:', index, mfccs.shape)\n",
    "        prob_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "ImVwh1C9EDv-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "MFCCs: [[-9.8337433e+02 -8.6239093e+02 -8.7461609e+02 ... -9.6640649e+02\n",
      "  -9.5538092e+02 -9.7637097e+02]\n",
      " [ 7.3092896e+01  7.3710541e+01  6.2771656e+01 ...  4.1964104e+01\n",
      "   6.5123352e+01  4.8438751e+01]\n",
      " [ 4.3509502e+00 -1.1370981e+00 -2.6839967e+00 ...  1.0083340e+01\n",
      "   1.5607588e+01  1.4771362e+01]\n",
      " ...\n",
      " [-5.8871088e+00 -1.3188517e+00  2.3336666e+00 ...  5.6511869e+00\n",
      "   4.4412464e-01 -2.1316152e+00]\n",
      " [ 1.3874447e+00  1.5675896e+00 -9.7566214e+00 ... -4.8497584e-01\n",
      "   1.7370390e+00 -2.3408337e+00]\n",
      " [ 3.3631542e+00  2.8551254e+00 -2.3733077e+00 ...  5.9705108e-01\n",
      "   4.2792058e+00  3.1438589e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d61959b048>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 내가 확인 할 부분\n",
    "idx = 1\n",
    "\n",
    "# Create path from given filename and target item\n",
    "path = '/'.join([dataset_path, target_list[int(y_orig_train[idx])], \n",
    "            filenames_train[idx]])\n",
    "\n",
    "# mfcc 만들기\n",
    "mfccs = get_librosa_mfcc(path)\n",
    "print(\"MFCCs:\", mfccs)\n",
    "\n",
    "# MFCC 그림으로 보기\n",
    "fig = plt.figure()\n",
    "plt.imshow(mfccs, cmap='inferno', origin='lower')\n",
    "\n",
    "# 소리 확인\n",
    "# print(target_list[int(y_orig_train[idx])])\n",
    "# print(path)\n",
    "# playsound(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "2O53qDlmF6pp"
   },
   "outputs": [],
   "source": [
    "# 학습 하기 전에 미리 모든 소리에 대해서 feature 추출해서 준비 해두는 단계, inpu을 맞추려고 \n",
    "def extract_features(in_files, in_y):\n",
    "    prob_cnt = 0\n",
    "    out_x = []\n",
    "    out_y = []\n",
    "        \n",
    "    for index, filename in enumerate(in_files):\n",
    "    \n",
    "        # Create path from given filename and target item\n",
    "        path = \"/\".join([dataset_path, target_list[int(in_y[index])], \n",
    "                    filename])\n",
    "        \n",
    "        # Check to make sure we're reading a .wav file\n",
    "        if not path.endswith('.wav'):\n",
    "            continue\n",
    "\n",
    "        # Create MFCCs\n",
    "        mfccs = get_librosa_mfcc(path)\n",
    "        print(mfccs.shape)\n",
    "\n",
    "        if mfccs.shape[1] == len_mfcc:\n",
    "            out_x.append(mfccs)\n",
    "            out_y.append(in_y[index])\n",
    "        else:\n",
    "            print('Dropped:', index, mfccs.shape)\n",
    "            prob_cnt += 1\n",
    "            \n",
    "    return out_x, out_y, prob_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "7\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_val))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "TVOZN9PqGHe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(40, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "(20000,)\n",
      "(40, 26)\n",
      "Train 잃은거0.0\n",
      "Valid 잃은거0.0\n",
      "Test 잃은거0.0\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, prob_train = extract_features(filenames_train, y_orig_train)\n",
    "x_val, y_val, prob_val = extract_features(filenames_val, y_orig_val)\n",
    "x_test, y_test, prob_test = extract_features(filenames_test, y_orig_test)\n",
    "\n",
    "\n",
    "print(\"Train 잃은거{}\".format(prob_train / len(y_orig_train)))\n",
    "print(\"Valid 잃은거{}\".format(prob_val / len(y_orig_val)))\n",
    "print(\"Test 잃은거{}\".format(prob_test / len(y_orig_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "BFEyc1kIG2oC"
   },
   "outputs": [],
   "source": [
    "# 학습에 사용할 모든 - MFCC 까지 모두 진행한 정보들\n",
    "#저장 할 곳\n",
    "np.savez(\"./mfcc_set.npz\", \n",
    "         x_train=x_train, \n",
    "         y_train=y_train, \n",
    "         x_val=x_val, \n",
    "         y_val=y_val, \n",
    "         x_test=x_test, \n",
    "         y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVQZDw7giaeS"
   },
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UORwgpYuFs3a",
    "outputId": "916b60f5-4a5f-48d5-8de7-1c98daf7477b"
   },
   "outputs": [],
   "source": [
    "# 나중에 저장할 모델 - 2진 분류\n",
    "model_filename = 'hi_yutan_original_model.h5'\n",
    "wake_word = 'hi_yutan'\n",
    "feature_sets = np.load( base_path + \"mfcc_set.npz\")\n",
    "\n",
    "# 저장되어 있는 mfcc feature 들 불러 오기\n",
    "x_train = feature_sets['x_train']\n",
    "y_train = feature_sets['y_train']\n",
    "x_val = feature_sets['x_val']\n",
    "y_val = feature_sets['y_val']\n",
    "x_test = feature_sets['x_test']\n",
    "y_test = feature_sets['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "iOdWs4rVoNX-",
    "outputId": "52a9eae8-a96b-4e60-a663-b856a489dbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 40, 26)\n",
      "(12, 40, 26)\n",
      "(24, 40, 26)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "gNjK8_proNIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 내가 수행 하고 싶은 \"기동어\" 설정\n",
    "# index (나는 하나니까 0) 이면, True 또는 False 반환 해주기 - 1아니면 0 ]\n",
    "wake_word_index = target_list.index(wake_word)\n",
    "y_train = np.equal(y_train, wake_word_index).astype('float64')\n",
    "y_val = np.equal(y_val, wake_word_index).astype('float64')\n",
    "y_test = np.equal(y_test, wake_word_index).astype('float64')\n",
    "print(wake_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "eFsT1UyyoNbG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 40, 26, 1)\n",
      "(12, 40, 26, 1)\n",
      "(24, 40, 26, 1)\n",
      "(40, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "# CNN 에 넣기 이전에 Channel을 1로 만들어주기\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], \n",
    "                          x_train.shape[1], \n",
    "                          x_train.shape[2], \n",
    "                          1)\n",
    "x_val = x_val.reshape(x_val.shape[0], \n",
    "                      x_val.shape[1], \n",
    "                      x_val.shape[2], \n",
    "                      1)\n",
    "x_test = x_test.reshape(x_test.shape[0], \n",
    "                        x_test.shape[1], \n",
    "                        x_test.shape[2], \n",
    "                        1)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "sample_shape = x_test.shape[1:]\n",
    "print(sample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "TllWIRQ8NHt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_9 (Sequential)    (None, 4, 2, 64)          12544     \n",
      "_________________________________________________________________\n",
      "sequential_10 (Sequential)   (None, 1)                 32897     \n",
      "=================================================================\n",
      "Total params: 45,441\n",
      "Trainable params: 45,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 Conv-layer \n",
    "conv_layer = keras.Sequential([\n",
    "                             layers.Conv2D(32, (2,2), activation = 'relu', input_shape = sample_shape),\n",
    "                             layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                             layers.Conv2D(32, (2, 2), activation='relu'),\n",
    "                             layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                             layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "                             layers.MaxPooling2D(pool_size=(2, 2))\n",
    "                             ])\n",
    "# FC layer는 다른거 사용   --  Class 10 개 분류\n",
    "fc_layer = keras.Sequential([\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(64, activation = 'relu'),\n",
    "                             layers.Dropout(0.5),\n",
    "                             layers.Dense(1, activation = \"sigmoid\")\n",
    "                             ])\n",
    "\n",
    "model = keras.Sequential([conv_layer,\n",
    "                          fc_layer\n",
    "                          ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "wnQ9lmkPOWzo"
   },
   "outputs": [],
   "source": [
    "# Callback 함수 지정 해주기      학습하는 동안 설정해줄것\n",
    "early_stop = EarlyStopping(patience=30) \n",
    "mc = ModelCheckpoint(base_path + \"best_model/wake_up_word_model\", \n",
    "                     save_best_only=True,\n",
    "                     monitor = 'val_loss',\n",
    "                     verbose = 1,\n",
    "                     mode = 'min') \n",
    "reduce_lr  = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                               factor=0.5, \n",
    "                               patience=5\n",
    "                               ) \n",
    "\n",
    "#optimizer 조정 해주기\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "znxHqYi_OW55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 4.0920 - accuracy: 0.6941 - val_loss: 0.8726 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87264, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.8243 - accuracy: 0.6941 - val_loss: 0.7020 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87264 to 0.70199, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 2.5528 - accuracy: 0.6824 - val_loss: 0.5806 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70199 to 0.58063, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3099 - accuracy: 0.6824 - val_loss: 0.4684 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58063 to 0.46840, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.0652 - accuracy: 0.6471 - val_loss: 0.3617 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46840 to 0.36170, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 2.6118 - accuracy: 0.6706 - val_loss: 0.2748 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36170 to 0.27482, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.5874 - accuracy: 0.6118 - val_loss: 0.2478 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.27482 to 0.24783, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.5092 - accuracy: 0.7294 - val_loss: 0.2239 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24783 to 0.22389, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.2975 - accuracy: 0.6706 - val_loss: 0.2069 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22389 to 0.20694, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.1069 - accuracy: 0.6235 - val_loss: 0.2160 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20694\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4764 - accuracy: 0.6706 - val_loss: 0.2020 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20694 to 0.20198, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.8115 - accuracy: 0.7647 - val_loss: 0.1917 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20198 to 0.19165, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.4106 - accuracy: 0.7176 - val_loss: 0.1786 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19165 to 0.17858, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.8507 - accuracy: 0.8000 - val_loss: 0.1552 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.17858 to 0.15525, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0283 - accuracy: 0.7647 - val_loss: 0.1222 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15525 to 0.12221, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.9430 - accuracy: 0.7176 - val_loss: 0.1113 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.12221 to 0.11128, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6880 - accuracy: 0.7529 - val_loss: 0.1123 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11128\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6618 - accuracy: 0.7647 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11128\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6757 - accuracy: 0.7647 - val_loss: 0.1449 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11128\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5587 - accuracy: 0.8118 - val_loss: 0.1489 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11128\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5422 - accuracy: 0.7529 - val_loss: 0.1199 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11128\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4113 - accuracy: 0.8235 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11128 to 0.10885, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5109 - accuracy: 0.8000 - val_loss: 0.1056 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10885 to 0.10558, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5111 - accuracy: 0.7647 - val_loss: 0.1025 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10558 to 0.10246, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4157 - accuracy: 0.8118 - val_loss: 0.0961 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10246 to 0.09610, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4605 - accuracy: 0.8235 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09610 to 0.09323, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3091 - accuracy: 0.9176 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09323 to 0.09056, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4349 - accuracy: 0.8118 - val_loss: 0.0925 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09056\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2172 - accuracy: 0.9059 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09056\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2078 - accuracy: 0.8941 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09056\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2956 - accuracy: 0.8471 - val_loss: 0.1230 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09056\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2833 - accuracy: 0.8353 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09056\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4149 - accuracy: 0.8353 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09056\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.2640 - accuracy: 0.8824 - val_loss: 0.0985 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09056\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3140 - accuracy: 0.8353 - val_loss: 0.0881 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09056 to 0.08808, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2552 - accuracy: 0.8588 - val_loss: 0.0822 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.08808 to 0.08219, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2477 - accuracy: 0.8941 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.08219 to 0.07992, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4346 - accuracy: 0.8118 - val_loss: 0.0809 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07992\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3163 - accuracy: 0.8471 - val_loss: 0.0796 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.07992 to 0.07959, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2279 - accuracy: 0.9059 - val_loss: 0.0787 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.07959 to 0.07868, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3138 - accuracy: 0.8824 - val_loss: 0.0775 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.07868 to 0.07749, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3525 - accuracy: 0.7882 - val_loss: 0.0789 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07749\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3231 - accuracy: 0.8941 - val_loss: 0.0792 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07749\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3774 - accuracy: 0.8000 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07749\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2526 - accuracy: 0.8588 - val_loss: 0.0832 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07749\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1628 - accuracy: 0.9176 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07749\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2203 - accuracy: 0.8941 - val_loss: 0.0853 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07749\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2890 - accuracy: 0.8588 - val_loss: 0.0849 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07749\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4108 - accuracy: 0.8235 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07749\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2542 - accuracy: 0.8824 - val_loss: 0.0825 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07749\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2969 - accuracy: 0.8471 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07749\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2138 - accuracy: 0.9059 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07749\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2792 - accuracy: 0.8824 - val_loss: 0.0774 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.07749 to 0.07738, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.2472 - accuracy: 0.8941 - val_loss: 0.0768 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.07738 to 0.07679, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2230 - accuracy: 0.8941 - val_loss: 0.0768 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.07679 to 0.07678, saving model to ./best_model\\wake_up_word_model\n",
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model\\wake_up_word_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2326 - accuracy: 0.8824 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07678\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2754 - accuracy: 0.8353 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07678\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2134 - accuracy: 0.8824 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07678\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2074 - accuracy: 0.9294 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07678\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1707 - accuracy: 0.9412 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07678\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2215 - accuracy: 0.9176 - val_loss: 0.0792 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07678\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2061 - accuracy: 0.9059 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07678\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1840 - accuracy: 0.9176 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07678\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1713 - accuracy: 0.9529 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07678\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.2190 - accuracy: 0.9059 - val_loss: 0.0783 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07678\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2718 - accuracy: 0.8824 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07678\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2104 - accuracy: 0.9176 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07678\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2333 - accuracy: 0.9176 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07678\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2230 - accuracy: 0.9059 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07678\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3137 - accuracy: 0.8471 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07678\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3110 - accuracy: 0.8824 - val_loss: 0.0778 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07678\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1959 - accuracy: 0.9294 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07678\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.2732 - accuracy: 0.8706 - val_loss: 0.0775 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07678\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2306 - accuracy: 0.9059 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07678\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2660 - accuracy: 0.8941 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07678\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2005 - accuracy: 0.9412 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07678\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2502 - accuracy: 0.8941 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07678\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2570 - accuracy: 0.8941 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07678\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1986 - accuracy: 0.9059 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07678\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1872 - accuracy: 0.9059 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07678\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1581 - accuracy: 0.9529 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07678\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2676 - accuracy: 0.9059 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07678\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2620 - accuracy: 0.8588 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07678\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1788 - accuracy: 0.9412 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07678\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2442 - accuracy: 0.8824 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07678\n"
     ]
    }
   ],
   "source": [
    "# optimizer, loss 함수를 정의하고,  학습 준비를 한다,  metrics 는 어떤 일이 발생하는지 보여줄 것들\n",
    "model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 한번에 몇개의 데이터 학습하고 가중치 갱신할지 \n",
    "history = model.fit(x_train, y_train,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          batch_size=32,\n",
    "          #validation_split = 0.1\n",
    "          validation_data = (x_val, y_val),\n",
    "          callbacks = [early_stop, reduce_lr , mc]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "id": "QGP-e_TnOW8-"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr0UlEQVR4nO3deZxU5Z3v8c+vmwZsFpFFRRpoXInK3ohKNLhkxoXBJWjk9lWJC0KcuCbGyEQZjXcmN0zGMUYzROMWFL2aOC6oiQtBY6I2SIwoRlTANi5NI5uAbL/7xzlFV5e1nOqu7lr6+369zquqTp0651enqn7nqed5znPM3RERkeJXlu8AREQkN5TQRURKhBK6iEiJUEIXESkRSugiIiVCCV1EpEQooUtSZvakmZ2b62XzycxWmNnxbbBeN7P9w/u/MLMfRlm2BdupNbPftTTONOudYGb1uV6vtL9O+Q5AcsfMNsY9rAS+AHaEjy9y97lR1+XuJ7bFsqXO3afnYj1mVg28D1S4+/Zw3XOByJ+hdDxK6CXE3bvH7pvZCuACd38mcTkz6xRLEiJSOlTl0gHE/lKb2ffN7GPgTjPbw8weN7MGM/ssvF8V95oFZnZBeH+qmb1oZrPDZd83sxNbuOwQM1toZhvM7Bkz+7mZ/TpF3FFivMHM/hiu73dm1jfu+bPNbKWZNZrZzDT7Z5yZfWxm5XHzTjOz18P7h5nZn8xsrZl9ZGa3mFnnFOu6y8x+FPf4e+Fr/m5m5yUse7KZvWZm683sAzObFff0wvB2rZltNLMjYvs27vVHmtmrZrYuvD0y6r5Jx8y+Er5+rZktNbNJcc+dZGZvhuv80My+G87vG34+a81sjZm9YGbKL+1MO7zj2BvoDQwGphF89neGjwcBm4Fb0rx+HPA20Bf4v8AdZmYtWPY+4BWgDzALODvNNqPE+L+AbwF7Ap2BWII5GLgtXP8+4faqSMLdXwY+B45NWO994f0dwOXh+zkCOA74dpq4CWM4IYzn68ABQGL9/efAOUAv4GRghpmdGj53dHjby927u/ufEtbdG3gCuDl8bz8FnjCzPgnv4Uv7JkPMFcBjwO/C130HmGtmB4WL3EFQfdcDOBR4Lpx/JVAP9AP2Aq4BNK5IO1NC7zh2Ate5+xfuvtndG939YXff5O4bgBuBr6V5/Up3/6W77wDuBvoT/HAjL2tmg4CxwLXuvtXdXwQeTbXBiDHe6e5/c/fNwIPAyHD+ZOBxd1/o7l8APwz3QSr3A1MAzKwHcFI4D3df5O5/dvft7r4C+O8kcSRzZhjfG+7+OcEBLP79LXD3v7r7Tnd/PdxelPVCcAB4x93vDeO6H1gG/FPcMqn2TTqHA92Bfw8/o+eAxwn3DbANONjMerr7Z+6+OG5+f2Cwu29z9xdcA0W1OyX0jqPB3bfEHphZpZn9d1glsZ7gL36v+GqHBB/H7rj7pvBu9yyX3QdYEzcP4INUAUeM8eO4+5viYtonft1hQm1MtS2C0vjpZtYFOB1Y7O4rwzgODKsTPg7j+D8EpfVMmsUArEx4f+PM7PmwSmkdMD3iemPrXpkwbyUwIO5xqn2TMWZ3jz/4xa/3GwQHu5Vm9gczOyKc/xNgOfA7M3vPzK6O9jYkl5TQO47E0tKVwEHAOHfvSdNf/FTVKLnwEdDbzCrj5g1Ms3xrYvwoft3hNvukWtjd3yRIXCfSvLoFgqqbZcABYRzXtCQGgmqjePcR/EMZ6O67A7+IW2+m0u3fCaqi4g0CPowQV6b1Dkyo/961Xnd/1d1PIaiOeYSg5I+7b3D3K919X2AScIWZHdfKWCRLSugdVw+COum1YX3sdW29wbDEWwfMMrPOYenun9K8pDUxPgRMNLOvhg2Y15P5+34fcCnBgeP/JcSxHthoZkOBGRFjeBCYamYHhweUxPh7EPxj2WJmhxEcSGIaCKqI9k2x7vnAgWb2v8ysk5l9EziYoHqkNV4mKM1fZWYVZjaB4DOaF35mtWa2u7tvI9gnOwHMbKKZ7R+2lawjaHdIV8UlbUAJveO6CdgNWA38GXiqnbZbS9Cw2Aj8CHiAoL98MjfRwhjdfSlwMUGS/gj4jKDRLp1YHfZz7r46bv53CZLtBuCXYcxRYngyfA/PEVRHPJewyLeB681sA3AtYWk3fO0mgjaDP4Y9Rw5PWHcjMJHgX0wjcBUwMSHurLn7VoIEfiLBfr8VOMfdl4WLnA2sCKuephN8nhA0+j4DbAT+BNzq7s+3JhbJnqndQvLJzB4Alrl7m/9DECl1KqFLuzKzsWa2n5mVhd36TiGoixWRVtKZotLe9gZ+Q9BAWQ/McPfX8huSSGlQlYuISIlQlYuISInIW5VL3759vbq6Ol+bFxEpSosWLVrt7v2SPZe3hF5dXU1dXV2+Ni8iUpTMLPEM4V1U5SIiUiKU0EVESoQSuohIiVA/dJEOZNu2bdTX17Nly5bMC0tede3alaqqKioqKiK/RgldpAOpr6+nR48eVFdXk/r6JJJv7k5jYyP19fUMGTIk8uuKqspl7lyoroaysuB2ri6XK5KVLVu20KdPHyXzAmdm9OnTJ+t/UkVTQp87F6ZNg03hpRFWrgweA9TWpn6diDSnZF4cWvI5FU0JfebMpmQes2lTMF9ERIoooa9ald18ESk8jY2NjBw5kpEjR7L33nszYMCAXY+3bt2a9rV1dXVccsklGbdx5JFH5iTWBQsWMHHixJysq70UTUIflHjxrgzzRaT1ct1u1adPH5YsWcKSJUuYPn06l19++a7HnTt3Zvv27SlfW1NTw80335xxGy+99FLrgixikRO6mZWb2Wtm9qVLXJlZFzN7wMyWm9nLZlad0yiBG2+Eysrm8yorg/kiknuxdquVK8G9qd0q150Rpk6dyvTp0xk3bhxXXXUVr7zyCkcccQSjRo3iyCOP5O233waal5hnzZrFeeedx4QJE9h3332bJfru3bvvWn7ChAlMnjyZoUOHUltbS2x02fnz5zN06FDGjBnDJZdckrEkvmbNGk499VSGDx/O4Ycfzuuvvw7AH/7wh13/MEaNGsWGDRv46KOPOProoxk5ciSHHnooL7zwQm53WBrZNIpeCrwF9Ezy3PnAZ+6+v5mdBfwY+GYO4tsl1vA5c2ZQzTJoUJDM1SAq0jbStVvl+ndXX1/PSy+9RHl5OevXr+eFF16gU6dOPPPMM1xzzTU8/PDDX3rNsmXLeP7559mwYQMHHXQQM2bM+FKf7ddee42lS5eyzz77MH78eP74xz9SU1PDRRddxMKFCxkyZAhTpkzJGN91113HqFGjeOSRR3juuec455xzWLJkCbNnz+bnP/8548ePZ+PGjXTt2pU5c+bwj//4j8ycOZMdO3awKXEntqFICd3MqoCTCa5xeEWSRU4BZoX3HwJuMTPzHA+2XlurBC7SXtqz3eqMM86gvLwcgHXr1nHuuefyzjvvYGZs27Yt6WtOPvlkunTpQpcuXdhzzz355JNPqKqqarbMYYcdtmveyJEjWbFiBd27d2fffffd1b97ypQpzJkzJ218L7744q6DyrHHHktjYyPr169n/PjxXHHFFdTW1nL66adTVVXF2LFjOe+889i2bRunnnoqI0eObM2uyUrUKpebCC5Cm+oq3gOADwDcfTvBVb/7JC5kZtPMrM7M6hoaGrKPVkTaTXu2W3Xr1m3X/R/+8Iccc8wxvPHGGzz22GMp+2J36dJl1/3y8vKk9e9RlmmNq6++mttvv53Nmzczfvx4li1bxtFHH83ChQsZMGAAU6dO5Z577snpNtPJmNDNbCLwqbsvau3G3H2Ou9e4e02/fkmH8xWRApGvdqt169YxYMAAAO66666cr/+ggw7ivffeY8WKFQA88MADGV9z1FFHMTdsPFiwYAF9+/alZ8+evPvuuwwbNozvf//7jB07lmXLlrFy5Ur22msvLrzwQi644AIWL16c8/eQSpQS+nhgkpmtAOYBx5rZrxOW+RAYCGBmnYDdgcYcxiki7ay2FubMgcGDwSy4nTOn7as9r7rqKn7wgx8watSonJeoAXbbbTduvfVWTjjhBMaMGUOPHj3Yfffd075m1qxZLFq0iOHDh3P11Vdz9913A3DTTTdx6KGHMnz4cCoqKjjxxBNZsGABI0aMYNSoUTzwwANceumlOX8PqWR1TVEzmwB8190nJsy/GBjm7tPDRtHT3f3MdOuqqalxXeBCpH299dZbfOUrX8l3GHm3ceNGunfvjrtz8cUXc8ABB3D55ZfnO6wvSfZ5mdkid69JtnyL+6Gb2fVmNil8eAfQx8yWEzSaXt3S9YqItLVf/vKXjBw5kkMOOYR169Zx0UUX5TuknMiqhJ5LKqGLtD+V0ItLu5XQRUSksCihi4iUCCV0EZESoYQuIlIilNBFpN0cc8wxPP30083m3XTTTcyYMSPlayZMmECsA8VJJ53E2rVrv7TMrFmzmD17dtptP/LII7z55pu7Hl977bU888wzWUSfXCENs6uELiLtZsqUKcybN6/ZvHnz5kUaIAuCURJ79erVom0nJvTrr7+e448/vkXrKlRK6CLSbiZPnswTTzyx62IWK1as4O9//ztHHXUUM2bMoKamhkMOOYTrrrsu6eurq6tZvXo1ADfeeCMHHnggX/3qV3cNsQtBH/OxY8cyYsQIvvGNb7Bp0yZeeuklHn30Ub73ve8xcuRI3n33XaZOncpDDz0EwLPPPsuoUaMYNmwY5513Hl988cWu7V133XWMHj2aYcOGsWzZsrTvL9/D7BbNNUVFJLcuuwyWLMntOkeOhJtuSv187969Oeyww3jyySc55ZRTmDdvHmeeeSZmxo033kjv3r3ZsWMHxx13HK+//jrDhw9Pup5FixYxb948lixZwvbt2xk9ejRjxowB4PTTT+fCCy8E4F/+5V+44447+M53vsOkSZOYOHEikydPbrauLVu2MHXqVJ599lkOPPBAzjnnHG677TYuu+wyAPr27cvixYu59dZbmT17NrfffnvK95fvYXZVQheRdhVf7RJf3fLggw8yevRoRo0axdKlS5tVjyR64YUXOO2006isrKRnz55MmjRp13NvvPEGRx11FMOGDWPu3LksXbo0bTxvv/02Q4YM4cADDwTg3HPPZeHChbueP/300wEYM2bMrgG9UnnxxRc5++yzgeTD7N58882sXbuWTp06MXbsWO68805mzZrFX//6V3r06JF23VGohC7SQaUrSbelU045hcsvv5zFixezadMmxowZw/vvv8/s2bN59dVX2WOPPZg6dWrKYXMzmTp1Ko888ggjRozgrrvuYsGCBa2KNzYEb2uG37366qs5+eSTmT9/PuPHj+fpp5/eNczuE088wdSpU7niiis455xzWhWrSugi0q66d+/OMcccw3nnnberdL5+/Xq6devG7rvvzieffMKTTz6Zdh1HH300jzzyCJs3b2bDhg089thju57bsGED/fv3Z9u2bbuGvAXo0aMHGzZs+NK6DjroIFasWMHy5csBuPfee/na177WoveW72F2VUIXkXY3ZcoUTjvttF1VL7HhZocOHcrAgQMZP3582tePHj2ab37zm4wYMYI999yTsWPH7nruhhtuYNy4cfTr149x48btSuJnnXUWF154ITfffPOuxlCArl27cuedd3LGGWewfft2xo4dy/Tp01v0vmLXOh0+fDiVlZXNhtl9/vnnKSsr45BDDuHEE09k3rx5/OQnP6GiooLu3bvn5EIYGpxLpAPR4FzFRYNziYh0UEroIiIlIso1Rbua2Stm9hczW2pm/5pkmalm1mBmS8LpgrYJV0RaK1/VrJKdlnxOURpFvwCOdfeNZlYBvGhmT7r7nxOWe8Dd/znrCESk3XTt2pXGxkb69OmDmeU7HEnB3WlsbKRr165ZvS5jQvfgMLExfFgRTjrEixShqqoq6uvraWhoyHcokkHXrl2pqqrK6jWRui2aWTmwCNgf+Lm7v5xksW+Y2dHA34DL3f2DJOuZBkwDGDRoUFaBikjrVVRUMGTIkHyHIW0kUqOou+9w95FAFXCYmR2asMhjQLW7Dwd+D9ydYj1z3L3G3Wv69evXirBFRCRRVr1c3H0t8DxwQsL8Rnf/Inx4OzAmJ9GJiEhkUXq59DOzXuH93YCvA8sSlukf93AS8FYOYxQRkQii1KH3B+4O69HLgAfd/XEzux6oc/dHgUvMbBKwHVgDTG2rgEVEJDmd+i8iUkR06r+ISAeghC4iUiKU0EVESoQSuohIiVBCFxEpEUroIiIlQgldRKREKKGLiJQIJXQRkRKhhC4iUiKU0EVESoQSuohIiVBCFxEpER0moc+dC9XVUFYW3M6dm++IRERyK9I1RYvd3LkwbRps2hQ8XrkyeAxQW5u/uEREcqlDlNBnzmxK5jGbNgXzRURKRZRL0HU1s1fM7C9mttTM/jXJMl3M7AEzW25mL5tZdZtE20KrVmU3X0SkGEUpoX8BHOvuI4CRwAlmdnjCMucDn7n7/sB/Aj/OaZStNGhQdvNFRIpRxoTugY3hw4pwSrxu3SnA3eH9h4DjzMxyFmUr3XgjVFY2n1dZGcwXESkVkerQzazczJYAnwK/d/eXExYZAHwA4O7bgXVAnyTrmWZmdWZW19DQ0KrAs1FbC3PmwODBYBbczpmjBlERKS1ZXSTazHoBvwW+4+5vxM1/AzjB3evDx+8C49x9dap16SLRIiLZy9lFot19LfA8cELCUx8CA8ONdQJ2BxqzjlRERFosSi+XfmHJHDPbDfg6sCxhsUeBc8P7k4HnPJuiv4iItFqUE4v6A3ebWTnBAeBBd3/czK4H6tz9UeAO4F4zWw6sAc5qs4hFRCSpjAnd3V8HRiWZf23c/S3AGbkNTUREslHUZ4pqfBYRkSZFO5aLxmcREWmuaEvoGp9FRKS5ok3oGp9FRKS5ok3oGp9FRKS5ok3oGp9FRKS5ok3oGp9FRKS5ou3lAkHyVgIXEQkUbQldRESaU0IXESkRHTah6yxTESk1HTKhx84yXbkS3IPbs88OGleV3EWkWHXIhJ7sLNPYYL+xIQSU1EWk2JRUQo9ajZLpbFINISAixahkEnqyapRUJe0oZ5NqCAERKTZRrlg00MyeN7M3zWypmV2aZJkJZrbOzJaE07XJ1tWWshmsK9lZpok0hICIFJsoJxZtB65098Vm1gNYZGa/d/c3E5Z7wd0n5j7EaLIZrCt2MtLMmUFJ3qypDh00hICIFKeMJXR3/8jdF4f3NwBvAQPaOrBsZTtYV20trFgRJPJ779UQAiJS/LKqQzezaoLL0b2c5OkjzOwvZvakmR2Si+Cy0ZrBumLJfefO4FbJXESKUeSEbmbdgYeBy9x9fcLTi4HB7j4C+BnwSIp1TDOzOjOra2hoaGHIyWmwLhHp6MzjK49TLWRWATwOPO3uP42w/Aqgxt1Xp1qmpqbG6+rqsghVRETMbJG71yR7LkovFwPuAN5KlczNbO9wOczssHC9jS0PWUREshWll8t44Gzgr2a2JJx3DTAIwN1/AUwGZpjZdmAzcJZHKfqLiEjOZEzo7v4iYBmWuQW4JVdBiYhI9krmTFERkY6uZBO6hscVkY6mqC9Bl0psXJfYUACxcV1A3RhFpHSVZAk9m3FdRERKRUkm9GzGdRERKRUlmdCzHddFRKQUlGRCb824LiIixaokE7rGdRGRjqgke7lAkLyVwEWkIynJErqISEekhC4iUiKU0LOkM1BFpFCVbB16W9AZqCJSyFRCz4LOQBWRQqaEngWdgSoihUwJPQs6A1VEClmUS9ANNLPnzexNM1tqZpcmWcbM7GYzW25mr5vZ6LYJN790BqqIFLIoJfTtwJXufjBwOHCxmR2csMyJwAHhNA24LadRFgidgSoihSzKJeg+Aj4K728ws7eAAcCbcYudAtwTXkf0z2bWy8z6h68tKToDVUQKVVZ16GZWDYwCXk54agDwQdzj+nBe4uunmVmdmdU1NDRkGaqIiKQTOaGbWXfgYeAyd1/fko25+xx3r3H3mn79+rVkFSIikkKkhG5mFQTJfK67/ybJIh8CA+MeV4XzRESknUTp5WLAHcBb7v7TFIs9CpwT9nY5HFhXivXnIiKFLMqp/+OBs4G/mtmScN41wCAAd/8FMB84CVgObAK+lfNIRUQkrSi9XF4ELMMyDlycq6BERCR7OlNURKREKKGLiJQIJXQRkRKhhC4iUiKU0EVESoQSuohIiVBCFxEpEUroIiIlQgldRKREKKGLiJQIJXQRkRKhhC4iUiKU0EVESoQSuohIiVBCFxEpEUroScydC9XVUFYW3M6dm++IREQyi3IJul+Z2adm9kaK5yeY2TozWxJO1+Y+zPYzdy5MmwYrV4J7cDttmpK6iBS+KCX0u4ATMizzgruPDKfrWx9W/sycCZs2NZ+3aVMwPxmV5kWkUES5BN1CM6tuh1gKwqpV0efHSvOxA0CsNA9QW9s28YmIpJKrOvQjzOwvZvakmR2SaiEzm2ZmdWZW19DQkKNN59agQdHnZ1uaFxFpS7lI6IuBwe4+AvgZ8EiqBd19jrvXuHtNv379crDp3LvxRqisbD6vsjKYnyhTaV7VMSLSnlqd0N19vbtvDO/PByrMrG+rI8uT2lqYMwcGDwaz4HbOnORVKOlK82pcFZH21uqEbmZ7m5mF9w8L19nY2vXmU20trFgBO3cGt6nqw9OV5lUdIyLtLUq3xfuBPwEHmVm9mZ1vZtPNbHq4yGTgDTP7C3AzcJa7e9uFXDjSleazaVwVEckFy1furamp8bq6urxsuz1UVwfVLIkGDw5K/SIiLWFmi9y9JtlzOlO0jWTTuCoikgtK6Bm0tKdKNo2rIiK5kPHEoo6stScO1dYqgYtI+1EJPQ31VBGRYqKEnoZ6qohIMVFCTyObYQBERPJNCT0N9VQRkWKihJ6GeqqISDFRL5cM1FNFRIqFSugiIiVCCV1EpEQooYuIlAgldBGREqGELiJSIpTQ80SXpxORXItygYtfmdmnZvZGiufNzG42s+Vm9rqZjc59mKVFl6cTkbYQpYR+F3BCmudPBA4Ip2nAba0Pq7Rp0C8RaQsZE7q7LwTWpFnkFOAeD/wZ6GVm/XMVYCnSoF8i0hZyUYc+APgg7nF9OO9LzGyamdWZWV1DQ0MONl2cUg3uVVamOnURabl2bRR19znuXuPuNf369WvPTReUZIN+AezYoTp1EWm5XCT0D4GBcY+rwnmSQuKgX+XlX15Gdeoikq1cJPRHgXPC3i6HA+vc/aMcrLek1dbCihWwc2cwJaM6dRHJRsbRFs3sfmAC0NfM6oHrgAoAd/8FMB84CVgObAK+1VbBlqpBg4JqlmTzRUSiitLLZYq793f3Cnevcvc73P0XYTIn7N1ysbvv5+7D3L2urYP+9NO23kLbSHUykS6kISK5UHRnit53H+yzDyxfnu9IspPuZCJdSENEcqHoEvoxxwQl3FtuyXckX5budP5MJxPF16mvWKFkLiLZK7qE3r8/nHkm/OpXsH59vqNpkul0fp1MJCJtregSOsCll8KGDXD33fmOpEmmEniqBk41fIpIrhRlQh87Fg4/HH72s9Rd/tpbphK4Gj5FpK0VZUIHuOQSeOcdeOqpfEcSyFQCV8OniLS1ok3okycHvV1uvjnfkQSilMDV8CkibaloE3pFBcyYAU8/DcuW5Tua3JbAM138QhfHEJFkzN3zsuGamhqvq2vdOUiffgoDB8J558FtJTIKe6y3THwDa0UF9OwJa9ZA795Bg/DWrU3PV1aq+kakozCzRe5ek+y5oi2hA+y5J0ydCrffDm++me9ociNZb5lt26CxMegO2djYPJmDBvISkUBRJ3SAG26A7t3h4ouDhFfsWtovXf3ZRaToE/qee8K//RssWAD335/vaFqvpf3S1Z9dRIo+oQNceCHU1MAVV8C6dfmOpnVSXfwiHfVnFxEokYReXh40in76KVx7bb6jaZ3E3jJ9+kDnzs2XqagI5see3203OPts9XgR6ehKIqFDUEKfPj0YtOu11/IdTevE91dfvToYtya+O+Sddwbz770XNm9uajBduRK+9S3o21ddGkU6oqLutpjos89g6FAYMABefjkoyZay6urkF8aIpy6NIqWl1d0WzewEM3vbzJab2dVJnp9qZg1mtiScLmht0C2xxx5w661BCX327HxE0L6i9GyJ79KYeELSt7+tE5RESknGErqZlQN/A74O1AOvAlPc/c24ZaYCNe7+z1E33BYl9JgzzoBHH4UlS+ArX2mTTRSEKCV0CKpq7r33yycsJVJpXqTwtbaEfhiw3N3fc/etwDzglFwGmGu33BL0TT//fNixI9/RtJ2oPWIGDUp+wlKidKV5ld5FCl+UhD4A+CDucX04L9E3zOx1M3vIzAYmW5GZTTOzOjOra2hoaEG40ey1VzBo15/+FAyxW6qi9IiJdWmMeuLRqlWZL9ZRaMm+0OIRyRt3TzsBk4Hb4x6fDdySsEwfoEt4/yLguUzrHTNmjLelnTvdTz7Zfbfd3N9+u003VVB+/Wv3wYPdzYLbGTOC2yA1R5vKy5PPHzw4WH9lZfP5lZXB/NbG2tJ15CqetpKL9ykSA9R5qnyd6oldC8ARwNNxj38A/CDN8uXAukzrbeuE7u7+4Yfue+zhPm6c+7Ztbb65gpMs2bVmiiWkVMm+tbG1JBHnKp62UgwHHCkurU3onYD3gCFAZ+AvwCEJy/SPu38a8OdM622PhO7ufv/9wbu84YZ22VxBSVcyb0npPVbKTJXscxFbtok4V/Ekk4uSdaEfcKT4tCqhB6/nJIKeLu8CM8N51wOTwvv/BiwNk/3zwNBM62yvhO7uftZZ7p06uS9a1G6bLAhRk12q5ZKVKgstEbc2nlRJO1cl67Y84EjH1OqE3hZTeyb0xkb3ffZxP/hg982b222zeRc12aVarry87RJdIVTdpHttruJTCV1yrcMndHf3p54K3u3FF7frZvMqarKLslx8SbZPn2Bq68bMqFUeqRqCk70uftlUjb+p5rekZK069Oxl83l2RErooSuvDN7xrbe2+6bzpqVJMTEJtqYUHCW55mqb6V6Xi0billTlZDoAqhdMkyifUUc/ICqhh7Zvd584MSiBPfVUu2++aLW02iDZj7OiIlrpvqXbTPe6bLtvtjSRZHMwyrRsS5N9IRwkWhJD1M8o9j0ohPfZ3pTQ46xf7z58uHvPnu5vvJGXEIpOSxv2ovw4UyWvTD/mVD/cdLFGafxNt82of/0zHYyiVPu0ps9/IVTztDSGqJ+RWeur7XJVjdjeVURK6AlWrnTfay/36mr3J5/smH3Us9HS0nLUH2eq5JVuSlXST9fAm2pdscbfXCXXdAeVqO8zXZ//VAe1dI25UT6vXMrmoBbl80u2nkyN+X36uHfunPwzy/Q5tObfWLp15eIfhRJ6Eq+84t63b7AH9t47qF9fvDg4w1Say6ZaIL6kky6JZpO8okwtrSdPfB8t7fUStdTd2vcZP8Uf1JIlr3T7OmrDcUureaLEkey9JHsu1WfW0n9cUT+HKAfAbA5AufrnpISewpYt7r/5jfuppwZfKHDfbz/3q65yf/ll9x078h1h4cimv3ZLfmCtqQ5pSXKN/5FFeZ+tLXW3Ngm19ZTugBiLOVOjeq7PSo5tM/Hg05LhLFoTR7oDYDZVRLnqwpouoZfUBS5aY/Vq+O1v4eGH4dlnYfv2YHz1I48MpmHDgkvcrVwZXE2oogJGjoRRo2DECOjRI9/vID+iDOFbXh5cfal3b9iwAbZubXouNmTvzJnJ1zN4cHAbdZjgnTubzysrC342UZZNJ9X7TBdf7H0PGhQMkFZbm3o95eWFMTJopjgqKqBnT1izJnhfJ50E8+dH+3yyNXhw8FuLFxs4LtPIoW0t0/c2G9l+F9MNn5s0y7fHVAgl9FTWrHG/5x738893Hzq0+dG0rMx94MCm6prYVFXl/rWvBa/58Y/d5893r68v/SqcqH+vY7Ip6WdbjZKspNOWJzBleu/JGo3Tvc9CLb3nopTb0tdF/TwhehVfLqeoVV3pplyW0JPObI+pkBN6otWr3f/4R/f33nPfujWYt3NnkLAfe8z9Rz9yP+cc9yOPdN9zz+Yf1h57uE+a5H7nncF6Sk2u6iLds+uRkKqxK9k6cz06ZNQklep9Z9sg2JJG4/j32V7VE+n2QUtiSLb/sq36iq+bTzUltv1kG2diA302VUKqQy8Ca9a4/+EP7rfc4n7hhe6DBgV7urzc/dhj3R98sHU9a3budF+2zH3hQvfHH3e/7z733/42P8Ma5Kq3QEu2G7XhLtd9lbPtjhlVto3PiQe1VD1/cl2/ne0BJVUMseSZzQE6171nsvlXl6uDT+y9q5dLkdq5072uzn3mTPd99w32enW1+003ua9bF20dn33m/j//4z5tWlC9k+xL0ru3+6WXtn//+kz9eT//3P3jj903bSqNKqi2+qG6t92BKlVf6VTvI1Z90ZqeJFEbnLN5L+3RN78lB8Bsqoda2300XUJXo2g727EDHnsM/uM/4MUXg3mDBsGBB8IBB0C3bkGD7Pbt8Pnn8M478PbbELvAU/fu8A//ACeeGDSw9ewZTKtWwR13BA2727YFDUoHHRSsd//9oVevYN3duwe3lZXBbbduwZWOolzKLp1162D58iDev/0tuP/uu/Dee/Dxx03LxRrVqqpgv/2Caf/9m26rqoKGuUKWroE0sRGv0CVrZIy/tuzcuUHD36pVyRu1E7XHdWnjY4pvcM7l6xKXzdT4G7UBNxf7J12jqBJ6Hr3yCjz1VFMSfOcd+OKLIKF16gRduwZJ7sADg+Q8ejQcddSXLzMXr6Eh+CK9+mqwzrffDn6EmXTrBnvuCX37NiX82LTbbsFt167BgWbr1iDOtWuDhP3uu9DY2Hx9AwcGSXrffYOpV68gjnXrgtetWhW87v33myeIzp2D5fffvynR9+8fxNWvX9DzaLfdgli6dAles2NH09SpU3DQMMvyw0hj587gR/n557BxY9AT6rrrYMuWpmW6dIErr4Tjjw+2XVbWdFte3jR17hzE17lz8B5iB9V8HsRykeiyTa7FKtsk3dKDTzpK6B2Ye5BsN2wIktHnnzdNGzcG05o1QZfMTz8Num9u2tSUwDZvbj516hQkry5dgq6aQ4Y0Je4DDgim/fYLkm4UO3dCfX2Q3GMl/Nj95ctb3j2trCyItVOnpgNk/FRWFvyTiU07dgT7KrbPdu4Mph07goNYW+vSJUj08TGWlzc/IJSVNR0oEg9YO3c2j3v79qaDXPz92PvM9LOP30bsfvyUbH5sXuJtYqyZHid7Ltn62nqbqR6vXx8UnLZvDz6nfv2Cf53ZuPDCoADQEukSeqeWrVKKhVlQuu3bN9+RJFdWFpRcBg2CY45p/pw7fPJJMK1eHfyIPvss+HewZUtTCTmWsMvKmqqr4hN1fFKLPb9jR5BAY1On8JcQ+/HGEmlZWfB8rKqqW7egZB3/2vjEG4s78aAQm7ZtC/6RbN0axB9/gI3FHYsx/rWxdcWmxP0Un+zj/xHE9k3848QknCg+4SerBU42PzYv8TbxwJHpcbLnkq2vrbcZZdnW6N+/bdYbKaGb2QnAfxFcL/R2d//3hOe7APcAY4BG4JvuviK3oUpHYwZ77x1MIpJZWaYFzKwc+DlwInAwMMXMDk5Y7HzgM3ffH/hP4Me5DlRERNLLmNCBw4Dl7v6eu28F5gGnJCxzCnB3eP8h4DizXDZLiYhIJlES+gDgg7jH9eG8pMu4+3ZgHdAncUVmNs3M6sysriHWD09ERHIiSkLPGXef4+417l7Tr1+/9ty0iEjJi5LQPwQGxj2uCuclXcbMOgG7EzSOiohIO4mS0F8FDjCzIWbWGTgLeDRhmUeBc8P7k4HnPF8d3EVEOqiM3RbdfbuZ/TPwNEG3xV+5+1Izu55gTIFHgTuAe81sObCGIOmLiEg7itQP3d3nA/MT5l0bd38LcEZuQxMRkWzk7dR/M2sAsrnWR19gdRuFUwq0fzLTPkpP+yezQthHg909aa+SvCX0bJlZXarxC0T7Jwrto/S0fzIr9H3Urt0WRUSk7Sihi4iUiGJK6HPyHUCB0/7JTPsoPe2fzAp6HxVNHbqIiKRXTCV0ERFJQwldRKREFHxCN7MTzOxtM1tuZlfnO55CYGYDzex5M3vTzJaa2aXh/N5m9nszeye83SPfseaTmZWb2Wtm9nj4eIiZvRx+lx4Ih7LosMysl5k9ZGbLzOwtMztC36EmZnZ5+Pt6w8zuN7Ouhf4dKuiEHvHiGh3RduBKdz8YOBy4ONwvVwPPuvsBwLPh447sUuCtuMc/Bv4zvBDLZwQXZunI/gt4yt2HAiMI9pW+Q4CZDQAuAWrc/VCCYU/OosC/QwWd0Il2cY0Ox90/cvfF4f0NBD/EATS/0MjdwKl5CbAAmFkVcDJwe/jYgGMJLsAC2j+7A0cTjMOEu29197XoOxSvE7BbOIJsJfARBf4dKvSEHuXiGh2amVUDo4CXgb3c/aPwqY+BvfIVVwG4CbgKiF1SuQ+wNrwAC+i7NARoAO4Mq6VuN7Nu6DsEgLt/CMwGVhEk8nXAIgr8O1ToCV3SMLPuwMPAZe6+Pv65cPjiDtkn1cwmAp+6+6J8x1LAOgGjgdvcfRTwOQnVKx38O7QHwb+VIcA+QDfghLwGFUGhJ/QoF9fokMysgiCZz3X334SzPzGz/uHz/YFP8xVfno0HJpnZCoJqumMJ6ot7hX+fQd+leqDe3V8OHz9EkOD1HQocD7zv7g3uvg34DcH3qqC/Q4We0KNcXKPDCeuD7wDecvefxj0Vf6GRc4H/ae/YCoG7/8Ddq9y9muA785y71wLPE1yABTrw/gFw94+BD8zsoHDWccCb6DsUswo43Mwqw99bbP8U9Heo4M8UNbOTCOpDYxfXuDG/EeWfmX0VeAH4K011xNcQ1KM/CAwiGJr4THdfk5cgC4SZTQC+6+4TzWxfghJ7b+A14H+7+xd5DC+vzGwkQaNxZ+A94FsEhTx9hwAz+1fgmwS9yl4DLiCoMy/Y71DBJ3QREYmm0KtcREQkIiV0EZESoYQuIlIilNBFREqEErqISIlQQhcRKRFK6CIiJeL/Ay1UZB6+XGreAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 과정 점검 하기\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "YliEVeZhO3TU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for idx, y in enumerate(y_test):\n",
    "    if y == 1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "dz9_EYVFPWya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D5E24F8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D5E24F8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 0.0  Prediction: [[0.15034735]]\n",
      "Answer: 0.0  Prediction: [[0.42904747]]\n",
      "Answer: 1.0  Prediction: [[0.97209847]]\n",
      "Answer: 1.0  Prediction: [[0.9646726]]\n",
      "Answer: 1.0  Prediction: [[0.97209847]]\n"
     ]
    }
   ],
   "source": [
    "# 학습한거에서 test 어떻게 나오는지 확인 해보기\n",
    "model = tf.keras.models.load_model(base_path + 'best_model/wake_up_word_model')\n",
    "for i in range(0, 5):\n",
    "    print('Answer:', y_test[i], ' Prediction:', model.predict(np.expand_dims(x_test[i], 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "pvyb-5m8PldX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1094 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10943608731031418, 1.0]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 40, 26, 1) (24,)\n",
      "[0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape, y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeQMo6BUPpHn"
   },
   "source": [
    "# tflite 변환 해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54HAWoFDPoOp"
   },
   "outputs": [],
   "source": [
    "tflite_filename = 'wake_word_hi_yutan_lite.tflite'\n",
    "\n",
    "model = tf.keras.models.load_model(base_path + 'best_model/wake_up_word_model')\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(tflite_filename, 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3vFUtQZRCN1"
   },
   "source": [
    "# 추론 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "Dztxh2RURmNs",
    "outputId": "7dfbfcd6-dbfb-479d-c3b8-b7f226c079f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.11.tar.gz (37 kB)\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (setup.py): started\n",
      "  Building wheel for pyaudio (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyaudio\n",
      "Failed to build pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "    Running setup.py install for pyaudio: started\n",
      "    Running setup.py install for pyaudio: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-wheel-lophh8k5'\n",
      "       cwd: C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-install-zgpxs60p\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\n",
      "  Complete output (9 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  copying src\\pyaudio.py -> build\\lib.win-amd64-3.7\n",
      "  running build_ext\n",
      "  building '_portaudio' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyaudio\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-record-lpmmfva0\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\Include\\pyaudio'\n",
      "         cwd: C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-install-zgpxs60p\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\n",
      "    Complete output (9 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    copying src\\pyaudio.py -> build\\lib.win-amd64-3.7\n",
      "    running build_ext\n",
      "    building '_portaudio' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\yutan\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zgpxs60p\\\\pyaudio_880cf6d2742844508cfcbcf7ff1b29e5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\yutan\\AppData\\Local\\Temp\\pip-record-lpmmfva0\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\Include\\pyaudio' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28824/3671692615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 기본 형태\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install pyaudio'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'"
     ]
    }
   ],
   "source": [
    "# 기본 형태\n",
    "import pyaudio\n",
    "import numpy as np\n",
    " \n",
    "CHUNK = 2**10  # 음성 데이터 불러올때, 한번에 몇개의 정수를 불러올지\n",
    "RATE = 44100   # 음성 데이터의 sampleing rate , mfcc 에 나오는 거라 다른거니까 구분 해두기\n",
    "\n",
    "# 음성 데이터 스트리을 여는 코드\n",
    "\"\"\"\n",
    "foramt : 비트 깊이를 설정 ,   여기서는 16bit가 된다\n",
    "input : 우리가 지금 열려고 하는 것이기 떄문에 True\n",
    "frames_per_buffer : 한번에 몇개의 정수를 불러올지\n",
    "input_device_index : 원하는 입력 장치의 번호 (이거 없으면 자동으로 설정 해준다..)\n",
    "\"\"\"\n",
    "p=pyaudio.PyAudio()\n",
    "stream=p.open(format=pyaudio.paInt16,channels=1,rate=RATE,input=True,\n",
    "              frames_per_buffer=CHUNK,input_device_index=2)\n",
    " \n",
    "# 음성 데이터를 입력받아 출력하는 소스\n",
    "while(True):\n",
    "    data = np.fromstring(stream.read(CHUNK),dtype=np.int16)\n",
    "    print(int(np.average(np.abs(data))))\n",
    " \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "PR2BRCGCWHL3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sounddevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28824/170868512.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Connect a resistor and LED to board pin 8 and run this script.\n",
    "Whenever you say \"stop\", the LED should flash briefly\n",
    "\"\"\"\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import timeit\n",
    "import python_speech_features\n",
    "import RPi.GPIO as GPIO\n",
    "\n",
    "from tflite_runtime.interpreter import Interpreter\n",
    "\n",
    "# Parameters\n",
    "debug_time = 1\n",
    "debug_acc = 0\n",
    "led_pin = 8\n",
    "word_threshold = 0.5\n",
    "rec_duration = 0.5 # 기동어 말 평균 길이로 하면 될듯 함\n",
    "window_stride = 0.5 # 이거는 유지 해주기\n",
    "sample_rate = 48000\n",
    "resample_rate = 8000\n",
    "num_channels = 1\n",
    "num_mfcc = 40\n",
    "model_path = base_path + 'wake_word_hi_yutan_lite.tflite'\n",
    "\n",
    "# Sliding window\n",
    "window = np.zeros(int(rec_duration * resample_rate) * 2)\n",
    "\n",
    "# GPIO \n",
    "GPIO.setwarnings(False)\n",
    "GPIO.setmode(GPIO.BOARD)\n",
    "GPIO.setup(8, GPIO.OUT, initial=GPIO.LOW)\n",
    "\n",
    "# Load model (interpreter)\n",
    "interpreter = Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "\n",
    "# Decimate (filter and downsample)\n",
    "def decimate(signal, old_fs, new_fs):\n",
    "    \n",
    "    # Check to make sure we're downsampling\n",
    "    if new_fs > old_fs:\n",
    "        print(\"Error: target sample rate higher than original\")\n",
    "        return signal, old_fs\n",
    "    \n",
    "    # We can only downsample by an integer factor\n",
    "    dec_factor = old_fs / new_fs\n",
    "    if not dec_factor.is_integer():\n",
    "        print(\"Error: can only decimate by integer factor\")\n",
    "        return signal, old_fs\n",
    "\n",
    "    # Do decimation\n",
    "    resampled_signal = scipy.signal.decimate(signal, int(dec_factor))\n",
    "\n",
    "    return resampled_signal, new_fs\n",
    "\n",
    "# This gets called every 0.5 seconds\n",
    "def sd_callback(rec, frames, time, status):\n",
    "\n",
    "    GPIO.output(led_pin, GPIO.LOW)\n",
    "\n",
    "    # Start timing for testing\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    # Notify if errors\n",
    "    if status:\n",
    "        print('Error:', status)\n",
    "    \n",
    "    # Remove 2nd dimension from recording sample\n",
    "    rec = np.squeeze(rec)\n",
    "    \n",
    "    # Resample\n",
    "    rec, new_fs = decimate(rec, sample_rate, resample_rate)\n",
    "    \n",
    "    # Save recording onto sliding window\n",
    "    window[:len(window)//2] = window[len(window)//2:]\n",
    "    window[len(window)//2:] = rec\n",
    "\n",
    "    # Compute features\n",
    "    mfccs = python_speech_features.base.mfcc(window, \n",
    "                                        samplerate=new_fs,\n",
    "                                        winlen=0.256,\n",
    "                                        winstep=0.050,\n",
    "                                        numcep=num_mfcc,\n",
    "                                        nfilt=26,\n",
    "                                        nfft=2048,\n",
    "                                        preemph=0.0,\n",
    "                                        ceplifter=0,\n",
    "                                        appendEnergy=False,\n",
    "                                        winfunc=np.hanning)\n",
    "    mfccs = mfccs.transpose()\n",
    "\n",
    "    # Make prediction from model\n",
    "    in_tensor = np.float32(mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1))\n",
    "    interpreter.set_tensor(input_details[0]['index'], in_tensor)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    val = output_data[0][0]\n",
    "    if val > word_threshold:\n",
    "        print('stop')\n",
    "        GPIO.output(led_pin, GPIO.HIGH)\n",
    "\n",
    "    if debug_acc:\n",
    "        print(val)\n",
    "    \n",
    "    if debug_time:\n",
    "        print(timeit.default_timer() - start)\n",
    "\n",
    "# Start streaming from microphone\n",
    "with sd.InputStream(channels=num_channels,\n",
    "                    samplerate=sample_rate,\n",
    "                    blocksize=int(sample_rate * rec_duration),  # 1초에 프레임 * 녹화 하는 기간\n",
    "                    callback=sd_callback):\n",
    "    while True:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDYLFa4VOdRPa2dkC4xL9I",
   "collapsed_sections": [
    "XVQZDw7giaeS",
    "SeQMo6BUPpHn"
   ],
   "include_colab_link": true,
   "mount_file_id": "1V3vMmYINqXBZajvEXLv_9LdNh_6e2wIN",
   "name": "Wake_up_word_all",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
